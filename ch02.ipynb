{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hlf401/nlpbook/blob/main/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3Mt3sehCDJf7"
   },
   "source": [
    "[[ch02]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6scHHUxDJf-"
   },
   "source": [
    "# Transformers and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MilnM5TXDJgA"
   },
   "source": [
    "Now that you've been introduced to the field of natural language processing, there's something imporant you need to understand. It's not actually a very long journey from where you start to state of the art.\n",
    "\n",
    "Eventually, we _will_ return to the basics, discuss the fundamentals, and understand all the details, of course. But we're going to show you the promised land before we venture on the long and hard journey to get there.\n",
    "    \n",
    "One of the most important ideas to implement if you want to get deep learning working in the real world is transfer learning, which is the process of taking a model that has already been trained on another dataset and fine-tuning it to fit your new dataset. For example, if you're training a language model to generate compelling short stories in the style of Hemingway, you could fine-tune a model trained on a wide variety of books instead of training on just the text samples of Hemingway, of which there may not be many.\n",
    "\n",
    ".Who's That Pokémon? Language Models\n",
    "> Tip: A language model is a function that takes in a sequence of words and returns a probility distribution over all the possible next words in that sequence. This task is considered one of the most important in NLP because, as the reasoning goes, to predict the next word in a sentence, you **must** have a good understanding of the language. Language models learn the features and characteristics of language in order to guess what the next word should be after any given prior phrase or sentence. Language models are the backbone of NLP today because they do not require explicit annotations (labels) and can be trained on massive corpuses without any material data preparation. Once they learn the properties of language well, language models can be fine-tuned to perform more specific NLP tasks such as text classification, which is exactly what we're going to do in this chapter.\n",
    "\n",
    "> Note: When we refer to pretrained models throughout this book, we are generally referring to large, pretrained *language* models that have been trained to perform language modeling on large corpuses.\n",
    "\n",
    "A nice analogy in object-oriented programming is the concept of inheritance in classes. Suppose we're making some sort of zoo management video game, where each animal is represented by a class. The animals have properties like weight and height, as well as functions like eat and sleep. In theory, we could just create a new class for each animal and replicate those shared functions, but in practice, we usually refactor our code so that we have a superclass for a generic animal and a subclass for each species to avoid duplication in our code, making it easier to read.\n",
    "\n",
    "By training on the larger dataset, the model essentially inherits a large amount of extra knowledge, which it can use to perform better on the task you care about. From a practical standpoint, transfer learning helps you get better performing models faster since fine-tuning, if done correctly, is often computationally cheaper than training from scratch.footnote:[Assuming that the original dataset you're transferring *from* is much larger than the dataset you're using for fine-tuning. If your fine-tuning dataset is larger, perhaps you should be applying transfer learning the other way around! But in practice, it's very hard to natural language text datasets that are of comparable size to the ones used for pretraining.]\n",
    "\n",
    "The other big advancement we'll discuss is the use of a new kind of model architecture called the transformer. Training transformers can be complicated and does not always work well without some fine-tuning. So, instead of traning it from scratch, we'll show you the pretraining technique on another architecure, and the use a popular pre-trained transformer to perform inference.\n",
    "\n",
    "For this chapter, it's important that you have your compute environment set up since we'll be training models. Check out our [Github page](https://github.com/nlpbook/nlpbook/) for more info on how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O53mZrVJDJgD"
   },
   "source": [
    "## Training with fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIQvrueiDJgD"
   },
   "source": [
    "The first thing we'll look at is an idea called transfer leaning. We're going to fine-tune a language model and then transform it into a text classifier that categorizes text based on sentiment. We'll start with the simplest working implementation, and progressively train our network using the [ULMFit](https://arxiv.org/abs/1801.06146) technique. This particular example was\n",
    "\n",
    "The dataset we're going to use here is the IMDB movie review datset. It's not very fun, but it's simple and small, which is what we want when starting off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BimPWCr2DJgE"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ru7DdsLMEAv"
   },
   "source": [
    "###挂载Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDduaVSDMLO-",
    "outputId": "abe16870-6f28-4c80-b4db-eefd2152f5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxBBUr3DMUTC"
   },
   "source": [
    "###symbol link到google drive-太慢，不好用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNcHxzDhMeK7",
    "outputId": "947a3294-acb3-4c9c-acdd-9c8135c6a376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lrwxrwxrwx 1 root root 35 Jan  9 03:53 /root/.fastai -> /content/drive/MyDrive/root/.fastai\n"
     ]
    }
   ],
   "source": [
    "#!ln -s /content/drive/MyDrive/root/.fastai /root/.fastai\n",
    "#!ls /root/.fastai -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bTihrGLDJgG"
   },
   "source": [
    "### Using the high-level fastai API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPC0IC-RDJgH"
   },
   "source": [
    "`fastai` is more more than your standard deep learning library. It includes tools that help you solve the problem at hand end-to-end as fast as possible. One of those tools is a built-in set of common datasets that can be easily downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "XH6ZfvP3DJgH",
    "outputId": "98abfa61-c46b-47dd-bfaa-a6b51df3afd3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [144441344/144440600 00:03&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "# downloading the datasets,   it may fail in China with vpn, when it failed , delete the folders:\n",
    "# C:\\Users\\Administrator\\.fastai\\archive and C:\\Users\\Administrator\\.fastai\\data, then run the codes to download again.\n",
    "# it is /root/.fastai/data/ on Colab\n",
    "# 下载IMDB压缩包到root\\.fastai\\archive\\imdb.tgz中\n",
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()\n",
    "print(\"abc\")\n",
    "#print(path.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQETa1K7DJgJ"
   },
   "source": [
    "This particular instance of the IMDB dataset is organized just like ImageNet is (i.e. one directory per class). So in this case, the positive reviews are saved under `pos` and the negative reviews are saved under `neg`.\n",
    "\n",
    "We can set up set up our dataset and prepare for training by using the `TextDataLoaders.from_folder` method built into `fastai`. The only thing we need to specify is the name of the validation folder, which is \"test\" (and not the default \"valid\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjqWIDwZDJgI",
    "outputId": "aa59ea21-d3c8-4e5d-8623-c77cfdc6dd02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/root/.fastai/data/imdb/train/labeledBow.feat'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znJ-fo8KbqNA"
   },
   "source": [
    "###数据集介绍\n",
    "The data follows an ImageNet-style organization, in the train folder, we have two subfolders, pos and neg (for positive reviews and negative reviews).\n",
    "\n",
    "\n",
    "We can gather it by using the TextDataLoaders.from_folder method. The only thing we need to specify is the name of the validation folder, which is “test” (and not the default “valid”).\n",
    "\"test\"是目录名字，用以指定测试数据集所在的文件目录。不指定默认为\"valid\"\n",
    "\n",
    "函数：TextDataLoaders.from_folder (path, train='train', valid='valid',...)\n",
    "说明：Create from imagenet style dataset in path with train and valid subfolders (or provide valid_pct)\n",
    "参考：\n",
    "https://docs.fast.ai/text.data.html#textdataloaders.from_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuk5cieDS-jI"
   },
   "source": [
    "###复制fastai创建的数据集IMDB到Google Drive里以查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vC3Dc8yRSZW"
   },
   "outputs": [],
   "source": [
    "!ls /root/.fastai/data/imdb/\n",
    "!tar -zcvf /content/drive/MyDrive/root/fastai.tgz /root/.fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nJa0XnnDJgJ",
    "outputId": "bc3c5de3-422b-48c7-c263-d3e285a9ac71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "# there is some warnings when running with Jupyter notebook on windows, you can ignore it,\n",
    "# the warning is \"Due to IPython and Windows limitation, python multiprocessing isn’t available now\"\n",
    "# it can work but very slowly, about 30 minutes for the first time\n",
    "#  fastai directly using Jupyter notebook in Win10 will occur this limit.\n",
    "# see https://forums.fast.ai/t/dataloaders-due-to-ipython-and-windows-limitation-python-multiprocessing-isnt-available-now/93906/2\n",
    "# see https://christianjmills.com/posts/fastai-book-notes/chapter-11/index.html  the source codes, for the warning details\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n",
    "# stored data in C:\\Users\\Administrator\\.fastai\\data\\imdb_tok\n",
    "#解压数据集imdb.tgz并处理分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRgLXf0cX7vg"
   },
   "outputs": [],
   "source": [
    "!ls /root/.fastai/data/imdb/\n",
    "!tar -zcvf /content/drive/MyDrive/root/fastai2.tgz /root/.fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO_q8cOoDJgK"
   },
   "source": [
    "We can then have a look at the data with the `show_batch` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "id": "TXYt4Y4sDJgK",
    "outputId": "3600ab25-2454-4ba7-d635-a9d6c3fc1675"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos i thought that xxup rotj was clearly the best out of the three xxmaj star xxmaj wars movies . i find it surprising that xxup rotj is considered the weakest installment in the xxmaj trilogy by many who have voted . xxmaj to me it seemed like xxup rotj was the best because it had the most profound plot , the most suspense , surprises , most xxunk the ending ) and definitely the most episodic movie . i personally like the xxmaj empire xxmaj strikes xxmaj back a lot also but i think it is slightly less good than than xxup rotj since it was slower - moving , was not as episodic , and i just did not feel as much suspense or emotion as i did with the third movie . \\n\\n xxmaj it also seems like to me that after reading these surprising reviews that</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxrep 3 * xxup spoilers xxrep 3 * xxrep 3 * xxup spoilers xxrep 3 * xxmaj continued … \\n\\n xxmaj from here on in the whole movie collapses in on itself . xxmaj first we meet a rogue program with the indication we 're gon na get ghosts and vampires and werewolves and the like . xxmaj we get a guy with a retarded accent talking endless garbage , two ' ghosts ' that serve no real purpose and have no character what - so - ever and a bunch of henchmen . xxmaj someone 's told me they 're vampires ( straight out of xxmaj blade 2 ) , but they 're so undefined i did n't realise . \\n\\n xxmaj the funny accented guy with a ridiculous name suffers the same problem as the xxmaj oracle , only for far longer and far far worse .</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj i 've rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i 've noticed that something is wrong with this movie ; it 's xxup terrible ! i mean , in the trailers it looked scary and serious ! \\n\\n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it 's funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night \" , \" the xxmaj lost xxmaj boys \" and \" the xxmaj return xxmaj of the xxmaj living xxmaj dead \" ! xxmaj those are funny ! \\n\\n \"</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxup myra xxup breckinridge is one of those rare films that established its place in film history immediately . xxmaj praise for the film was absolutely nonexistent , even from the people involved in making it . xxmaj this film was loathed from day one . xxmaj while every now and then one will come across some maverick who will praise the film on philosophical grounds ( aggressive feminism or the courage to tackle the issue of xxunk ) , the film has not developed a cult following like some notorious flops do . xxmaj it 's not hailed as a misunderstood masterpiece like xxup scarface , or trotted out to be ridiculed as a camp classic like xxup showgirls . \\n\\n xxmaj undoubtedly the reason is that the film , though outrageously awful , is not lovable , or even likable . xxup myra xxup breckinridge is just</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj my xxmaj comments for xxup vivah : - xxmaj its a charming , idealistic love story starring xxmaj shahid xxmaj kapoor and xxmaj amrita xxmaj rao . xxmaj the film takes us back to small pleasures like the bride and bridegroom 's families sleeping on the floor , playing games together , their friendly banter and mutual respect . xxmaj vivah is about the sanctity of marriage and the importance of commitment between two individuals . xxmaj yes , the central romance is naively visualized . xxmaj but the sneaked - in romantic moments between the to - be - married couple and their stubborn resistance to modern courtship games makes you crave for the idealism . xxmaj the film predictably concludes with the marriage and the groom , on the wedding night , tells his new bride who suffers from burn injuries : \" come let me</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxup warning : xxup possible xxup spoilers ( but not really - keep reading ) . a xxrep 3 h , there are so many reasons to become utterly addicted to this spoof gem that i wo n't have room to list them all . xxmaj the opening credits set the playful scene with kitsch late 1950s cartoon stills ; an enchanting xxmaj xxunk ' prez ' xxmaj xxunk mambo theme which appears to be curiously uncredited ( but his grunts are unmistakable , and no - one else did them ) ; and with familiar cast names , including xxmaj kathy xxmaj xxunk a full year before she hit with xxmaj sister xxmaj acts 1 &amp; 2 plus xxmaj teri xxmaj hatcher from tv 's xxmaj superman . \\n\\n xxmaj every scene is imbued with shallow injustices flung at various actors , actresses and producers in daytime xxup</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj pier xxmaj paolo xxmaj pasolini , or xxmaj pee - pee - pee as i prefer to call him ( due to his love of showing male genitals ) , is perhaps xxup the most overrated xxmaj european xxmaj marxist director - and they are thick on the ground . xxmaj how anyone can see \" art \" in this messy , cheap sex - romp concoction is beyond me . xxmaj some of the \" stories \" here could have come straight out of a soft - core porn film , and i am not even so much referring to the nudity but the simplistic and banal , often pointless stories . xxmaj anyone who enjoyed this relatively watchable but dumb oddity should really sink his teeth into the \" der xxmaj xxunk \" soft - porn xxmaj german 70s movie series , because that 's what</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj well , on it 's credit side ( if it can be said to have one ) , xxmaj timothy xxmaj hines xxup did manage to capture the original setting of xxup h.g . xxmaj wells ' outstanding novella . xxmaj but other than that - well , to call a spade a spade - it sucks bigtime . xxmaj what the xxmaj master xxmaj ed xxmaj wood could have done with the alleged $ 20 million dollar budget ! xxmaj timothy xxmaj hines really does make xxmaj mr . xxmaj wood , who was a flawed genius anyway , look like the best filmmaker of all time . xxmaj the special effects ( i guess you 'd call them that ) are not even up to computer game standards . xxmaj the acting is , well , perhaps about dinner theater comparable , and the accents are</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-mpNqQfjfe2"
   },
   "source": [
    "问题：\n",
    "*   show_batch()显示的数据从哪里提取的？\n",
    "*   9个是哪里定义的\n",
    "*   每次运行后结果不一致，看来随机从train集里跳出来的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVtJotNjDJgK"
   },
   "source": [
    "We can see that the library automatically processed all the texts to split then in *tokens*, adding some special tokens like:\n",
    "\n",
    "库自动添加的这些tokens 用来分词\n",
    "\n",
    "- `xxbos` to indicate the beginning of a text\n",
    "- `xxmaj` to indicate the next word was capitalized\n",
    "\n",
    "`fastai` uses an object called a `Learner` for doing pretty much everything. We can construct one for text classification in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Y4oR2L6CDJgL",
    "outputId": "6124d193-de3c-4546-c2cf-c880a99841e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 00:02&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "#创建分类器\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CaKsJAaDJgL"
   },
   "source": [
    "Instead of the transformer model that we've been raving about (and will continue to dicuss) throughout a vast majority of the book, we're going to use the [AWD LSTM](https://arxiv.org/abs/1708.02182) architecture instead for now, since it's easier and faster to train.\n",
    "\n",
    "There are a few other details: `drop_mult` is a parameter that controls the magnitude of all dropouts in that model, and we use `accuracy` to track down how well we are doing. But you don't need to worry too much about hyperparameters just yet.\n",
    "\n",
    "With the `Learner` defined, we can now fine-tune our pretrained model, using a method with an unsurprising name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "kWZDeVFtDJgM",
    "outputId": "b7d5079f-b0ab-4437-e87f-39b5aceff158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.408969</td>\n",
       "      <td>0.816640</td>\n",
       "      <td>03:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314508</td>\n",
       "      <td>0.267143</td>\n",
       "      <td>0.892320</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248217</td>\n",
       "      <td>0.199403</td>\n",
       "      <td>0.922320</td>\n",
       "      <td>07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194687</td>\n",
       "      <td>0.201378</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.141802</td>\n",
       "      <td>0.185537</td>\n",
       "      <td>0.932160</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "FV62wuVzDJgM",
    "outputId": "766327e9-e62c-4d2b-9108-f50a5b07ee8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.150610</td>\n",
       "      <td>0.201975</td>\n",
       "      <td>0.930040</td>\n",
       "      <td>03:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.178695</td>\n",
       "      <td>0.204791</td>\n",
       "      <td>0.921400</td>\n",
       "      <td>07:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168506</td>\n",
       "      <td>0.182030</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111449</td>\n",
       "      <td>0.197089</td>\n",
       "      <td>0.931200</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074513</td>\n",
       "      <td>0.221328</td>\n",
       "      <td>0.930720</td>\n",
       "      <td>06:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYrgh0B8DJgM"
   },
   "source": [
    "93% accuracy look good! But let's see how well it's actually doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "QUHF8sNoDJgN",
    "outputId": "33d820c8-469f-4a5f-8574-3cbb30d08a03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos ( some spoilers included :) \\n\\n xxmaj although , many commentators have called this film surreal , the term fits poorly here . xxmaj to quote from xxmaj encyclopedia xxmaj xxunk 's , surreal means : \\n\\n \" fantastic or incongruous imagery \" : xxmaj one need n't explain to the unimaginative how many ways a plucky ten - year - old boy at large and seeking his fortune in the driver 's seat of a red xxmaj mustang could be fantastic : those curious might read xxmaj james xxmaj kincaid ; but if you asked said lad how he were incongruous behind the wheel of a sports car , he 'd surely protest , \" no way ! \" xxmaj what fantasies and incongruities the film offers mostly appear within the first fifteen minutes . xxmaj thereafter we get more iterations of the same , in an ever</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is n't much different at all from the previous games ( excluding xxmaj tony xxmaj hawk 3 ) . xxmaj the only thing new that is featured in xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is the new selection of levels , and tweaked out graphics . xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x offers a new career mode , and that is the 2x career . xxmaj the 2x career is basically xxmaj tony xxmaj hawk 1 career , because there is only about five challenges per level . xxmaj if you missed xxmaj tony xxmaj hawk 1 and 2 , i suggest that you buy xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , but if you have played the first two games , you should still</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj warner xxmaj brothers tampered considerably with xxmaj american history in \" big xxmaj trail \" director xxmaj raoul xxmaj walsh 's first - rate western \" they xxmaj died with xxmaj their xxmaj boots xxmaj on , \" a somewhat inaccurate but wholly exhilarating biography of cavalry officer xxmaj george xxmaj armstrong xxmaj custer . xxmaj the film chronicles xxmaj custer from the moment that he arrives at xxmaj west xxmaj point xxmaj academy until the xxmaj indians massacre him at the xxmaj little xxmaj big xxmaj horn . xxmaj this is one of xxmaj errol xxmaj flynn 's signature roles and one of xxmaj raoul xxmaj walsh 's greatest epics . xxmaj walsh and xxmaj flynn teamed in quite often afterward , and \" they xxmaj died with xxmaj their xxmaj boots xxmaj on \" reunited xxmaj olivia de xxmaj havilland as xxmaj flynn 's romantic interest</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj may 2nd : someone clicked 11 nos , and then proceeded to do 15 more on my previous 15 comments : almost as funny as this turkey ! \\n\\n xxmaj may 1st : \\n\\n xxmaj as i write this , xxmaj i 'm still very much under the impression of what must be the funniest thriller xxmaj i 've ever seen . xxmaj i 've got a major case of the giggles , but xxmaj i 'll try and calm down . ( it 's kind of hard to write when your nose spills snot and the mouth ejects sporadic drool onto the keyboard . ) \\n\\n a pair of young women who just returned from a vacation take a ride on a shuttle bus . a couple of young guys join them . xxmaj but the bus is n't really a taxi service : it 's a</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj this is , per se , an above average film but why in the name of xxmaj bog was it made ? xxmaj it 's impossible to treat it as a thing unto itself because it is an almost shot - for - shot remake of an xxmaj alfred xxmaj hitchcock classic of 1960 . xxmaj you ca n't watch it without the 1960 film nudging into your consciousness . \\n\\n xxmaj what does the word \" credit \" mean ? xxmaj how can we credit xxmaj van xxmaj xxunk and his associates with anything except deciding to use different actors , slightly different sets , and color ? \\n\\n xxmaj anne xxmaj heche is attractive but lacks xxmaj janet xxmaj leigh 's stolid determination to become a respectable middle - class woman . xxmaj and xxmaj heche is younger than xxmaj leigh , who brought to her</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos by xxmaj dane xxmaj youssef \\n\\n i was kind of looking forward to this one . i enjoy xxmaj eddie xxmaj murphy and i love it when a star hand - makes a vehicle for themselves or when someone who writes decides to mark their own directorial debut . xxmaj but when the star 's head gets too big for the rest of his body , there 's always a danger of a big - budgeted xxmaj hollywood vanity production . \\n\\n xxmaj will the filmmaker keep it real ",
       "▁ or will he just waste amounts of money ( the studio 's , ours ) and time ( the studio 's , ours &amp; his own ) patting himself on the back for an hour in a half ? xxmaj sadly , it 's the latter here . \\n\\n xxmaj another thing i really like is when someone breathes</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos i do not think i am alone when i say that 2005 has not been particularly kind to the horror genre . xxmaj while \" cursed \" , \" hide and xxmaj seek \" , \" the xxmaj ring xxmaj two \" , and \" the xxmaj amityville xxmaj horror \" all showed glimpses of interest and potential , there have been more misses than hits . xxmaj for proof , see : \" white xxmaj noise \" , \" boogeyman \" , \" the xxmaj jacket \" , \" mindhunters \" , and \" alone in the xxmaj dark \" . xxmaj imagine my surprise when \" house of xxmaj wax \" , tightly written by siblings xxmaj chad and xxmaj carey xxmaj hayes , turned out to be … well , a surprise . \\n\\n xxmaj carly xxmaj jones ( elisha xxmaj cuthbert ) is a young</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj why would xxmaj burt xxmaj lancaster allow himself to play a poor schnook who is ultimately undermined by femme fatale xxmaj anna xxmaj dundee , played by xxmaj yvonne decarlo in ' criss xxmaj cross ' ? xxmaj the same reason why xxmaj robert xxmaj mitchum allows himself to be cast as another loser who falls for femme fatale xxmaj faith xxmaj domergue in the 1950 noir , \" where xxmaj danger xxmaj lives \" . xxmaj perhaps they both felt it was a good way to show that they had ' range ' as xxunk playing against type , the usual ' tough - guy ' role they were known for , would enhance their image as actors who could play any role . xxmaj but the problem was that roles like xxmaj steve xxmaj thompson , the pathetic love - sick milquetoast in ' criss xxmaj</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PReXcabW1QPM"
   },
   "source": [
    "**HLF COMMENTS：**\n",
    "\n",
    "每次输出结果，不一样，对不同的文本进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWnL5-UzDJgN"
   },
   "source": [
    "We can also run prediction on individual sentences one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOb8-op6s0XW"
   },
   "outputs": [],
   "source": [
    "!ls /root/.fastai/data/imdb/\n",
    "!tar -zcvf /content/drive/MyDrive/root/fastai3.tgz /root/.fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BcdhIn_jDJgN",
    "outputId": "b405164e-4178-4f05-fcc1-07b9992ec426"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('pos', tensor(1), tensor([0.0459, 0.9541]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"That movie was wicked cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ET8LXyysrw0o",
    "outputId": "7d90adfe-9ea4-42c7-fd34-f2b32aae13d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('neg', tensor(0), tensor([0.9976, 0.0024]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"That movie was stupid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BrTIkwj01nRW",
    "outputId": "a859b6e7-1e58-41a9-c2e8-98a5faffe11f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('pos', tensor(1), tensor([0.0037, 0.9963]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"That movie was very good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUSdUWO_DJgO"
   },
   "source": [
    "Here we can see the model has considered the review to be positive. The second part of the result is the index of \"pos\" in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for \"pos\" and 0.9% for \"neg\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0NOzWaGDJgO"
   },
   "source": [
    "#### Building a Dataset with fastai's DataBlock API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8xElQhmDJgO"
   },
   "source": [
    "We can also use the `fastai` data block API to get our data in a `DataLoaders`. This is a bit more advanced, so fell free to skip this part if you are not comfortable with `fastai` just yet. This approach will give us the same results in the end.\n",
    "\n",
    "A datablock is built by giving the fastai library a bunch of information:\n",
    "\n",
    "- the types used, through an argument called `blocks`: here we have images and categories, so we pass `TextBlock` and `CategoryBlock`. To inform the library our texts are files in a folder, we use the `from_folder` class method.\n",
    "- how to get the raw items, here our function `get_text_files`.\n",
    "- how to label those items, here with the parent folder.\n",
    "- how to split those items, here with the grandparent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_1-P1WSDJgO"
   },
   "outputs": [],
   "source": [
    "imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock),\n",
    "                 get_items=get_text_files,\n",
    "                 get_y=parent_label,\n",
    "                 splitter=GrandparentSplitter(valid_name='test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQqcMxj4DJgP"
   },
   "source": [
    "This only gives a blueprint on how to assemble the data. To actually create it, we need to use the `dataloaders` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpvndW_bDJgP"
   },
   "outputs": [],
   "source": [
    "dls = imdb.dataloaders(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOqsWN-DJgP"
   },
   "source": [
    "### ULMFiT for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH-7cklgDJgP"
   },
   "source": [
    "The pretrained model we used in the previous section is called a language model. It was trained to guess the next word on a set of Wikipedia articles after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better.\n",
    "\n",
    "The Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb dataset and then use *that* as the base for our classifier instead of the Wikipedia language model.\n",
    "\n",
    "This intuitivly makes sense - if you, as a literate human being, get some context on what movie review generally sound like, you'd probably do a better job of classifying them. It's kind of like getting the passage to read a few days in advance before you take the SAT. Only here, we won't call the language model out for cheating, since we're friends footnote:[See, I said it right here. Please don't eat me, robot overlords in the future.].\n",
    "\n",
    "But beyond that, another very important reason this is useful is because we often have more data for our than we have *labelled* data. Labelling is expensive and generally requires human time and effort, so it's not uncommon to have a large database of text record where only a small subset of them are used for say, document tagging. But with this fine-tuning approach, we can still use the unlabelled data to fine-tune the *language model* even before we train the\n",
    "\n",
    "At the risk of dragging on a flawed analogy, this is almost like getting access to years of previous SAT passages. None of them will show up on the test *exactly*, but practicing them will help get a sense of what the SAT is like.\n",
    "\n",
    "This approach is called ULMFiT, introducted by Jeremy Howard footnote:[Who also happends to be the creator of fastai!] and Sebastian Ruder in 2018. The process is summarized in [[ulmfit]]\n",
    "\n",
    "![ULMFit](https://github.com/hlf401/nlpbook/blob/main/images/ulmfit.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYWJBlQDDJgQ"
   },
   "source": [
    "Arrows and circles make everything so much simpler, don't they?\n",
    "\n",
    "Since we already have the pretrained Wikipedia language model, we can start with step 2 of the piple in [[ulmfit]] - fine-tuning the IMDB language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e_Bz6ouqsgN"
   },
   "source": [
    "添加：\n",
    "\n",
    "But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles.\n",
    "\n",
    "unsup folder里存放没有标记的文本。可以利用它predicting the next word of a movie review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tN0kpCeP4NN"
   },
   "source": [
    "###准备工作：import fastai and 挂载Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGpPN8pQQamD"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bD3DHMe0QdnS",
    "outputId": "74cf17c7-c7a3-41a0-f00f-7d385c364a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "nOyGYXDbR0VI",
    "outputId": "300d35c3-9244-4b76-c54a-113476b0b1d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [144441344/144440600 00:04&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/root/.fastai/data/imdb/train/labeledBow.feat'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# downloading the datasets,   it may fail in China with vpn, when it failed , delete the folders:\n",
    "# C:\\Users\\Administrator\\.fastai\\archive and C:\\Users\\Administrator\\.fastai\\data, then run the codes to download again.\n",
    "# it is /root/.fastai/data/ on Colab\n",
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()\n",
    "print(\"abc\")\n",
    "#print(path.ls())\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKhDEm48DJgQ"
   },
   "source": [
    "### Fine-tuning a language model on IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRfaqnpxDJgQ"
   },
   "source": [
    "We can get our texts in a `DataLoaders` suitable for language modeling very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "yxLkAZg8DJgQ",
    "outputId": "a011b669-212e-4c1f-ebf0-d4ac14139048"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oft79akGDJgR"
   },
   "source": [
    "We need to pass something for `valid_pct` otherwise this method will try to split the data by using the grandparent folder names. By passing `valid_pct=0.1`, we tell it to get a random 10% of those reviews for the validation set.\n",
    "\n",
    "We can have a look at our data using `show_batch`. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right.\n",
    "\n",
    "这里目标是预测后面的 words，不是分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBmP0rAmyrAi"
   },
   "source": [
    "**HLF COMMENTS::**\n",
    "\n",
    "\n",
    "https://docs.fast.ai/text.data.html#textdataloaders.from_folder\n",
    "\n",
    "textdataloaders.from_folder()\n",
    "\n",
    "\n",
    "If valid_pct is provided, a random split is performed (with an optional seed) by setting aside that percentage of the data for the validation set (instead of looking at the grandparents folder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "jJAtZxiyDJgR",
    "outputId": "b2a9f8b7-0dd1-4aa9-cf2e-53acead88580"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj first off . i would like to state i have never written a review in my life , let alone , one for imdb . i watched this movie and felt so strongly that it needed a fair review on this site . xxmaj there will be no major spoilers . \\n\\n xxmaj the movie centers on xxmaj christopher and xxmaj grace . xxmaj both of whom are in their</td>\n",
       "      <td>xxmaj first off . i would like to state i have never written a review in my life , let alone , one for imdb . i watched this movie and felt so strongly that it needed a fair review on this site . xxmaj there will be no major spoilers . \\n\\n xxmaj the movie centers on xxmaj christopher and xxmaj grace . xxmaj both of whom are in their early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>; their oddest touch was in having all of xxmaj sandra 's in - house audiences looking completely bored -- a feeling many real viewers will most likely share . * 1 / 2 from xxrep 4 * xxbos i do n't know , maybe the rest of the people voting on this movie were looking for an xxmaj academy award - winning movie , but my family was just looking for</td>\n",
       "      <td>their oddest touch was in having all of xxmaj sandra 's in - house audiences looking completely bored -- a feeling many real viewers will most likely share . * 1 / 2 from xxrep 4 * xxbos i do n't know , maybe the rest of the people voting on this movie were looking for an xxmaj academy award - winning movie , but my family was just looking for what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice to see xxmaj englund playing against type but sadly his role is a bit short to be enjoyed . xxmaj scott xxmaj wilson 's role as xxmaj leslie 's only friend xxmaj eugene is a hoot as a career killer in hiding . xxmaj but co - writer / director xxmaj glosserman does keeps the horror fans happy by adding in - jokes to your favorite slasher movies and he is</td>\n",
       "      <td>to see xxmaj englund playing against type but sadly his role is a bit short to be enjoyed . xxmaj scott xxmaj wilson 's role as xxmaj leslie 's only friend xxmaj eugene is a hoot as a career killer in hiding . xxmaj but co - writer / director xxmaj glosserman does keeps the horror fans happy by adding in - jokes to your favorite slasher movies and he is not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and over just does n't ever let up leaving the audience no time to settle in leaving them grasping for straws . \\n\\n xxmaj another problem is the settings and time frame . xxmaj the movie is set in modern day xxmaj xxunk xxmaj colorado . xxmaj every other xxmaj alien and xxmaj predator movie was shot in enthralling settings , such as industrial space ships , guerrilla war zone rainforest 's</td>\n",
       "      <td>over just does n't ever let up leaving the audience no time to settle in leaving them grasping for straws . \\n\\n xxmaj another problem is the settings and time frame . xxmaj the movie is set in modern day xxmaj xxunk xxmaj colorado . xxmaj every other xxmaj alien and xxmaj predator movie was shot in enthralling settings , such as industrial space ships , guerrilla war zone rainforest 's ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxmaj fante and xxmaj mingo by bringing them what looks like a box of money but is actually a stick of dynamite . xxmaj mingo is devastated when xxmaj fante is killed and agrees to testify against xxmaj brown , not for xxmaj diamond but for his beloved gay lover ( the xxmaj big xxmaj combo does win points for suggesting that xxmaj fante and xxmaj mingo are gay which is unusual</td>\n",
       "      <td>fante and xxmaj mingo by bringing them what looks like a box of money but is actually a stick of dynamite . xxmaj mingo is devastated when xxmaj fante is killed and agrees to testify against xxmaj brown , not for xxmaj diamond but for his beloved gay lover ( the xxmaj big xxmaj combo does win points for suggesting that xxmaj fante and xxmaj mingo are gay which is unusual for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCD3n0gVDJgR"
   },
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. `to_fp16` puts the `Learner` in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "gmT55B-hDJgR",
    "outputId": "e0190e5c-c29e-465a-f165-7b18fbf2088b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 00:03&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "#创建语言模型\n",
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()],\n",
    "    path=path, wd=0.1).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABpSNK9SO2Qq"
   },
   "source": [
    "**HLF COMMENTS**\n",
    "\n",
    "参考：https://docs.fast.ai/text.learner.html#language_model_learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xkgwIFCDJgX"
   },
   "source": [
    "By default, a pretrained `Learner` is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "C-swybbIDJgX",
    "outputId": "fb1ac68c-0601-445d-8303-edfa5bee5007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
      "/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.400478</td>\n",
       "      <td>4.114402</td>\n",
       "      <td>0.285941</td>\n",
       "      <td>61.215584</td>\n",
       "      <td>23:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练模型\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCN_95uaDJgY"
   },
   "source": [
    "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0oDop8NDJgY"
   },
   "source": [
    "You can easily save the state of your model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRJ64iq2DJgZ",
    "outputId": "1f6147bb-d28c-4155-c5c3-459d1925fa5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/root/.fastai/data/imdb/models/1epoch.pth')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存模型\n",
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3QERDrdqGzA"
   },
   "outputs": [],
   "source": [
    "生成的model在：Path('/root/.fastai/data/imdb/models/1epoch.pth')\n",
    "\n",
    "后面备份。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFeNFNXyqUq5"
   },
   "outputs": [],
   "source": [
    "#!mkdir -p /content/drive/MyDrive/root/model\n",
    "#!cp /root/.fastai/data/imdb/models/1epoch.pth /content/drive/MyDrive/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE4cJZliejBB"
   },
   "outputs": [],
   "source": [
    "!cp /root/.fastai/data/imdb/models/1epoch.pth /content/drive/MyDrive/root/ -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNQyKgPxO2Qt"
   },
   "outputs": [],
   "source": [
    "print(learn.path)\n",
    "!ls /root/.fastai/data/imdb/\n",
    "!tar -zcvf /content/drive/MyDrive/root/fastai4.tgz /root/.fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIUhMVy1DJgZ"
   },
   "source": [
    "It will create a file in `learn.path/models/` named \"1epoch.pth\". If you want to load your model on another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UHXnfbZbIR-"
   },
   "source": [
    "### 断开服务，以测试后面load model的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zyj8itsbVQk"
   },
   "source": [
    "###准备工作: import lib and mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsTKUZpaa_-7"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92oslckcbHQR",
    "outputId": "cf827174-5eda-4497-c678-76ca75094136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "WefGeNSyceGN",
    "outputId": "ab486646-00b1-429f-9e18-24d6aaa14ed1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [144441344/144440600 00:11&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/root/.fastai/data/imdb/train/labeledBow.feat'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading the datasets,   it may fail in China with vpn, when it failed , delete the folders:\n",
    "# C:\\Users\\Administrator\\.fastai\\archive and C:\\Users\\Administrator\\.fastai\\data, then run the codes to download again.\n",
    "# it is /root/.fastai/data/ on Colab\n",
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()\n",
    "print(\"abc\")\n",
    "#print(path.ls())\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mUZx65asb-i1",
    "outputId": "a1070d18-0f60-4f38-8c38-9c953df67502"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#加载数据集并预处理\n",
    "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "-7DInGaUcFOs",
    "outputId": "5a33cd52-1048-4fbe-8807-594409729744"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this film , like much of their music , is either underrated or unnoticed by the casual observer . xxmaj it is terrific and , in many ways , ahead of its time . \\n\\n xxmaj the images are funny , disturbing , and at the very least , engaging . \\n\\n xxmaj the music is amazing . \\n\\n xxmaj this is not the \" candy pop \" sound they</td>\n",
       "      <td>xxmaj this film , like much of their music , is either underrated or unnoticed by the casual observer . xxmaj it is terrific and , in many ways , ahead of its time . \\n\\n xxmaj the images are funny , disturbing , and at the very least , engaging . \\n\\n xxmaj the music is amazing . \\n\\n xxmaj this is not the \" candy pop \" sound they are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxmaj henry xxmaj hathaway 's fine direction and a wealth of good sense from 20th century - fox . xxmaj fox was still well - taken with their new cinemascope process that just begged for action and beautiful , colorful settings . xxmaj this movie excels at all , but it 's mostly the rock - solid story of xxmaj king xxmaj arthur and the xxmaj vikings that makes it . \\n\\n</td>\n",
       "      <td>henry xxmaj hathaway 's fine direction and a wealth of good sense from 20th century - fox . xxmaj fox was still well - taken with their new cinemascope process that just begged for action and beautiful , colorful settings . xxmaj this movie excels at all , but it 's mostly the rock - solid story of xxmaj king xxmaj arthur and the xxmaj vikings that makes it . \\n\\n xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. xxbos xxmaj from the very beginning , where xxmaj vic xxmaj damone sings the xxup beautiful xxmaj title xxmaj song , to the end where i sit teary - eyed , again , i xxup love this film . i also loved the original , \" sentimental xxmaj journey \" just as much . xxmaj both may be a bit unrealistic in today 's no - room - for - sentiment</td>\n",
       "      <td>xxbos xxmaj from the very beginning , where xxmaj vic xxmaj damone sings the xxup beautiful xxmaj title xxmaj song , to the end where i sit teary - eyed , again , i xxup love this film . i also loved the original , \" sentimental xxmaj journey \" just as much . xxmaj both may be a bit unrealistic in today 's no - room - for - sentiment world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>felt i had my money 's worth . xxmaj alas , for me their legend has faded . xxmaj they were more ad - xxunk than comic talents . xxbos xxmaj the actors in this dark film are truly believable and well cast . xxmaj the quality of the camera work makes you feel as if you are there xxmaj the screenplay is intense and does not wander . xxmaj the plot</td>\n",
       "      <td>i had my money 's worth . xxmaj alas , for me their legend has faded . xxmaj they were more ad - xxunk than comic talents . xxbos xxmaj the actors in this dark film are truly believable and well cast . xxmaj the quality of the camera work makes you feel as if you are there xxmaj the screenplay is intense and does not wander . xxmaj the plot is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brothers under the skin , so wo n't you just like me even though we 're different because i am xxunk to you \" politically correct party line . xxmaj this film avoids the clichés and the pathos and the racial grovelling and shows folks of all backgrounds stuck in a miserable hole together and just getting through the day . xxmaj everyone is facing a common enemy . xxmaj in this</td>\n",
       "      <td>under the skin , so wo n't you just like me even though we 're different because i am xxunk to you \" politically correct party line . xxmaj this film avoids the clichés and the pathos and the racial grovelling and shows folks of all backgrounds stuck in a miserable hole together and just getting through the day . xxmaj everyone is facing a common enemy . xxmaj in this film</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "6wVNqNlfcKnD",
    "outputId": "262f8889-da03-4dbf-be82-f74f3eab7c5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 00:08&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "#创建语言模型\n",
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()],\n",
    "    path=path, wd=0.1).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R83EFc4yb_1N"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /root/.fastai/data/imdb/models\n",
    "!cp /content/drive/MyDrive/root/1epoch.pth /root/.fastai/data/imdb/models/1epoch.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f04T0mY8fxwr",
    "outputId": "58e7d25c-2269-4428-e6b3-d65641393203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361196 -rw------- 1 root root 369858122 Jan  9 14:36 /root/.fastai/data/imdb/models/1epoch.pth\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.fastai/data/imdb/models/1epoch.pth -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19rnMMajDJgZ",
    "outputId": "f8e032e0-6c41-4b51-e3d8-6dae4ee088c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyV22WtzDJgZ"
   },
   "source": [
    "We can them fine-tune the model after unfreezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "Qakq3jtBDJga",
    "outputId": "982efd7e-0403-4427-8571-aa86bf000692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n",
      "/usr/local/lib/python3.10/dist-packages/fastai/callback/fp16.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.autocast,self.learn.scaler,self.scales = autocast(dtype=dtype),GradScaler(**self.kwargs),L()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.061425</td>\n",
       "      <td>3.866227</td>\n",
       "      <td>0.311518</td>\n",
       "      <td>47.761837</td>\n",
       "      <td>26:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#加载完模型  解冻后   再训练\n",
    "learn.unfreeze()\n",
    "#learn.fit_one_cycle(10, 1e-3)\n",
    "#learn.fit_one_cycle(3, 1e-3) # to reduce time\n",
    "learn.fit_one_cycle(1, 1e-3) # to reduce time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6LUU6QGDJga"
   },
   "source": [
    "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMHsahjLDJga"
   },
   "outputs": [],
   "source": [
    "#保存编码器\n",
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkVWdoMFqpMM"
   },
   "source": [
    "### 备份训练和微调好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6GHbKR-nOdb",
    "outputId": "d2386112-78d0-49a8-d990-2173f6c95b50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch.pth  finetuned.pth\n",
      "tar: Removing leading `/' from member names\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.fastai/data/imdb/models\n",
    "!tar -zcf /content/drive/MyDrive/root/fastai5.tgz /root/.fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcVvYGdQfJQJ"
   },
   "outputs": [],
   "source": [
    "#!cp /root/.fastai/data/imdb/models/finetuned.pth /content/drive/MyDrive/root/ -f\n",
    "!cp /root/.fastai/data/imdb/models/finetuned.pth /content/drive/MyDrive/root/finetuned2.pth -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSQ5xRSzDJga"
   },
   "source": [
    ".Who's That Pokémon?\n",
    "> Tip: The encoder is the model not including the task-specific final layer(s). It means much the same thing as *body* when applied to vision CNNs, but tends to be more used for NLP and generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnznk172DJgb"
   },
   "source": [
    "Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnLKmNAIDJgb"
   },
   "outputs": [],
   "source": [
    "#使用语言模型预测下一句测试\n",
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRHkFaXODJgc",
    "outputId": "20d92245-e717-42a1-9b18-ff820b4e5f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because it was so funny and funny . This movie is definitely a must see . It is hilarious and has some very funny moments . This movie is a great movie . It follows the lives\n",
      "i liked this movie because i was very surprised . The dialogue submarines , the flick is very well done , and the dialogue was very good . There were some flaws and some of the emotions were so badly made .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc9um_1_k5rx"
   },
   "outputs": [],
   "source": [
    "#使用语言模型预测下一句测试2\n",
    "TEXT = \"I did not like this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztPt7WRKk-8z",
    "outputId": "818cb835-5ab6-461e-e6de-bc94b3683647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i did not like this movie because i saw the trailer for it on TV and i was coronado on . The acting was very good and i loved the whole thing , so there was a lot of good action sequences . It\n",
      "i did not like this movie because i was in a working class but i ca n't wait to see it again . On the Internet , i thought it was bad , and i scarecrows my . It was like a horror movie\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD24gkkzDJgc"
   },
   "source": [
    "With the language model fine-tuned on movie review, we can now modify it to *classify* movie reviews. The idea is that at this point, if the model is \"smart enough\" to predict the next word, it *must* be able to a simple positive/negative classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDBjzrhaDJgc"
   },
   "source": [
    "### Training a text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V0zPzftxN94"
   },
   "source": [
    "### 断开服务，以测试后面load model的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5xhCI-MxN-I"
   },
   "source": [
    "###准备工作: import lib and mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T8aYM_RNxN-I"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpDovJgjxN-I",
    "outputId": "1064dc11-228d-4b4c-d26f-52f4d08c0f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "lHAMCIBfxN-J",
    "outputId": "49c52e69-a243-4e44-c799-25aa1bcfad34"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [144441344/144440600 00:45&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#4) [Path('/root/.fastai/data/imdb/train/labeledBow.feat'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/pos'),Path('/root/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading the datasets,   it may fail in China with vpn, when it failed , delete the folders:\n",
    "# C:\\Users\\Administrator\\.fastai\\archive and C:\\Users\\Administrator\\.fastai\\data, then run the codes to download again.\n",
    "# it is /root/.fastai/data/ on Colab\n",
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()\n",
    "print(\"abc\")\n",
    "#print(path.ls())\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcefi4rw1XTQ"
   },
   "source": [
    "###加载模型（预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWOIexW61YOs",
    "outputId": "6a3b30c6-34b9-4d9d-89c6-8d2bc74b6481"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "#load 数据集  并处理,用于语言模型\n",
    "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "v4EaRnLp1YO7",
    "outputId": "ae9dfb50-eb7b-4fc2-98bc-b4b1aea13c32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj there 's been a vogue for the past few years for often - as - not ironic zombie - related films , as well as other media incarnations of the xxunk eating resurrected dead . \" fido \" is a film that 's either an attempt to cash in on that , simply a manifestation of it , or both -- and it falls squarely into the category of ironic</td>\n",
       "      <td>xxmaj there 's been a vogue for the past few years for often - as - not ironic zombie - related films , as well as other media incarnations of the xxunk eating resurrected dead . \" fido \" is a film that 's either an attempt to cash in on that , simply a manifestation of it , or both -- and it falls squarely into the category of ironic zombies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to sing and dance about . xxmaj the film in the end will likely put you to sleep . \\n\\n xxmaj and , as a twisted punch in the face , this film is so pro the xxup us it makes me sick . xxmaj the movie keeps on saying again and again , the xxup us is xxmaj god and xxmaj russia is the devil . xxmaj this is the kind</td>\n",
       "      <td>sing and dance about . xxmaj the film in the end will likely put you to sleep . \\n\\n xxmaj and , as a twisted punch in the face , this film is so pro the xxup us it makes me sick . xxmaj the movie keeps on saying again and again , the xxup us is xxmaj god and xxmaj russia is the devil . xxmaj this is the kind of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. xxmaj this opening segment establishes the perfect mood for the rest of the film , which never takes itself too seriously and includes outrageous characters that seem self - aware of their own existence in this nonsensical gangster flick . \\n\\n xxmaj the film is broken into segments based on various events and times during the course of one day . xxmaj this effect is much like xxmaj pulp xxmaj fiction</td>\n",
       "      <td>xxmaj this opening segment establishes the perfect mood for the rest of the film , which never takes itself too seriously and includes outrageous characters that seem self - aware of their own existence in this nonsensical gangster flick . \\n\\n xxmaj the film is broken into segments based on various events and times during the course of one day . xxmaj this effect is much like xxmaj pulp xxmaj fiction ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports movie ( not the xxmaj rocky movies … happy xxmaj gilmore ! ) xxmaj overall , this is a goofy comedy . xxmaj at times , it 's funny , but more often than not , it 's just very annoying and predictable . \\n\\n xxmaj my rating : * 1 / 2 out of xxrep 4 * . 90 mins . xxup pg-13 for language , sexual humor and drug</td>\n",
       "      <td>movie ( not the xxmaj rocky movies … happy xxmaj gilmore ! ) xxmaj overall , this is a goofy comedy . xxmaj at times , it 's funny , but more often than not , it 's just very annoying and predictable . \\n\\n xxmaj my rating : * 1 / 2 out of xxrep 4 * . 90 mins . xxup pg-13 for language , sexual humor and drug humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faith in the process . xxmaj if you want to see something better i suggest \" the xxmaj prophecy \" with xxmaj christopher xxmaj walken . xxbos xxmaj while not as good as some of his previous films , xxmaj the xxmaj world is still an excellent movie . xxmaj imagine a soap opera director ( or a xxup tv ad director ) making xxmaj italian xxmaj neo - realism , and</td>\n",
       "      <td>in the process . xxmaj if you want to see something better i suggest \" the xxmaj prophecy \" with xxmaj christopher xxmaj walken . xxbos xxmaj while not as good as some of his previous films , xxmaj the xxmaj world is still an excellent movie . xxmaj imagine a soap opera director ( or a xxup tv ad director ) making xxmaj italian xxmaj neo - realism , and you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WtYUyzHDJgc"
   },
   "source": [
    "We can gather our data for text classification almost exactly like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO4AslUpDJgd",
    "outputId": "094cbb5d-98a0-47e3-cd6d-24d221657ba3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dls_clas = TextDataLoaders.from_folder(\n",
    "    untar_data(URLs.IMDB), valid='test',\n",
    "    text_vocab=dls_lm.vocab)\n",
    "'''\n",
    "#重新加在数据集  用于分类模型  并且使用预训练模型同样的词汇表\n",
    "dls_clas = TextDataLoaders.from_folder(\n",
    "    path, valid='test',\n",
    "    text_vocab=dls_lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUviHqAwDJgd"
   },
   "source": [
    "The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with `text_vocab`.\n",
    "\n",
    "函数：TextDataLoaders.from_folder (path, train='train', valid='valid',...)\n",
    "\n",
    "说明：Create from imagenet style dataset in path with train and valid subfolders (or provide valid_pct)\n",
    "\n",
    "参考： https://docs.fast.ai/text.data.html#textdataloaders.from_folder\n",
    "\n",
    "Then we can define our text classifier like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp9XsYFEDJgd",
    "outputId": "3a36f7f8-b309-44f1-e0d6-a92b188bd986"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n"
     ]
    }
   ],
   "source": [
    "#创建文本分类器\n",
    "#learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aouEzURDJgd"
   },
   "source": [
    "The difference is that before training it, we load the previous encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbAATdJex-U3",
    "outputId": "98eb29cd-ef5e-4f85-8c7b-3ddf7dba3a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch.pth   fastai3.tgz  fastai5.tgz  fastai.tgz      finetuned.pth  P51_data文件list.txt\n",
      "fastai2.tgz  fastai4.tgz  fastai6.tgz  finetuned2.pth  model\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/root/\n",
    "!mkdir -p /root/.fastai/data/imdb/models\n",
    "!cp /content/drive/MyDrive/root/finetuned.pth /root/.fastai/data/imdb/models/ -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwbDKHNEDJge",
    "outputId": "ae41ec16-3c32-4704-ff31-aaab4717c3c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "#加载编码器\n",
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nwGraBSDJge"
   },
   "source": [
    "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "dSsa0S6wDJge",
    "outputId": "794caec9-1034-4176-f4c6-4f13d70ce07c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.290510</td>\n",
       "      <td>0.240381</td>\n",
       "      <td>0.902360</td>\n",
       "      <td>03:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练一轮\n",
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfjmLSV8DJge"
   },
   "source": [
    "In just one epoch we get the same result as our training in the first section, not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "91oRsRZGDJgf",
    "outputId": "6f56f1ef-e06b-442f-85be-ac6bebdc6f4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.254611</td>\n",
       "      <td>0.196190</td>\n",
       "      <td>0.924840</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#解冻到倒数第二层 训练\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8sFf2ZiDJgf"
   },
   "source": [
    "Then we can unfreeze a bit more, and continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "fnxCHAeqDJgf",
    "outputId": "77a4e310-9046-4ba3-9677-668a5daab82f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.209723</td>\n",
       "      <td>0.170984</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>05:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#解冻到倒数第3层 训练\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ekMTO1vDJgf"
   },
   "source": [
    "Finally, we can unfreeze the entire model, and let it train all the layers to get a final boost in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "h_VK0KJKDJgg",
    "outputId": "fbe042a9-f59a-4485-d3b7-1112e67392ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.169802</td>\n",
       "      <td>0.169055</td>\n",
       "      <td>0.936720</td>\n",
       "      <td>07:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.140738</td>\n",
       "      <td>0.169266</td>\n",
       "      <td>0.936120</td>\n",
       "      <td>07:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#解冻全部   训练\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX0cR-Qs7lZL"
   },
   "source": [
    "###测试保存encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WxfOf4m7pWh",
    "outputId": "819ea40e-cbbd-4504-ca0d-30866e8f0bc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fastai/text/learner.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  wgts = torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "learn.save_encoder('finetuned_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56FdZMHp77cq",
    "outputId": "9921f928-c548-413a-fc2b-d33c28a35870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 518896\n",
      "172964 -rw-r--r-- 1 root root 177111138 Jan 10 06:58 finetuned3.pth\n",
      "172964 -rw-r--r-- 1 root root 177111223 Jan 10 06:51 finetuned_final.pth\n",
      "172968 -rw------- 1 root root 177111121 Jan 10 06:22 finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -ls /root/.fastai/data/imdb/models/\n",
    "!cp /root/.fastai/data/imdb/models/finetuned_final.pth /content/drive/MyDrive/root/ -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVGsAmBfyjQO"
   },
   "source": [
    "###分类测试输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "bdRQXKzByfR_",
    "outputId": "b3c90227-4395-4738-fc26-7670ac7d57f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj back in the mid / late 80s , an xxup oav anime by title of \" bubblegum xxmaj crisis \" ( which i think is a military slang term for when technical equipment goes haywire ) made its debut on video , taking inspiration from \" blade xxmaj runner \" , \" the xxmaj terminator \" and maybe even \" robocop \" , with a little dash of xxmaj batman / xxmaj bruce xxmaj wayne - xxmaj iron xxmaj man / xxmaj tony xxmaj stark and xxmaj charlie 's xxmaj angel 's girl power thrown in for good measure . 8 episodes long , the overall story was that in 21st century xxmaj tokyo , xxmaj japan , year xxunk - xxunk , living machines called xxmaj boomers were doing manual labor and sometimes cause problems . a special , xxup swat like branch of law enforcers ,</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj watching xxmaj stranger xxmaj than xxmaj fiction director xxmaj marc xxmaj forster 's xxmaj the xxmaj kite xxmaj runner is the cinematic equivalent of eating your vegetables because this art - house epic rated xxup pg-13 is good for your movie - going diet . xxmaj no , this is n't the kind of movie that i like to slouch on the couch and eyeball at the end of a tough day . xxmaj the xxmaj kite xxmaj runner is n't your typical mainstream movie designed to entertain you and make you forget about your troubles . xxmaj first , no celebrity stars appear in it . xxmaj second , nothing is cut and dried , black or white , or so outlandish that you do n't believe an image that you see . xxmaj third , xxmaj the xxmaj kite xxmaj runner lapses into subtitles when the</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos \" the xxmaj moon xxmaj is xxmaj blue \" director xxmaj otto xxmaj preminger tackled even more taboo subject matter in his controversial 1955 release \" the xxmaj man with the xxmaj golden xxmaj arm . \" xxmaj whereas he had incensed the xxmaj motion xxmaj picture xxmaj association of xxmaj america with his use of the words \" virgin \" and \" mistress \" in his mild 1953 comedy \" the xxmaj moon xxmaj is xxmaj blue , \" xxmaj preminger went far beyond what any movie had attempted with \" the xxmaj man with the xxmaj golden xxmaj arm \" since xxmaj dick xxmaj powell made his law and order epic \" to the xxmaj ends of the xxmaj xxunk ) about thwarting the international traffic in narcotics . xxmaj based on xxmaj nelson xxmaj algren 's novel that won the 1950 xxmaj national xxmaj book xxmaj</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj some people drift through life , moving from one thing or one person to the next without batting an eye ; others latch on to a cause , another person or a principle and remain adamant , committed to whatever it is-- and figuratively or literally they give their word and stand by it . xxmaj but we 're all different , ` made of different clay , ' as one of the characters in this film puts it , which is what makes life so interesting . xxmaj some people are just plain crazy , xxunk and maybe that 's the way you have to be to live among the masses . xxmaj who knows ? xxmaj who knows what it takes to make xxunk life-- work ? xxmaj writer / director xxmaj lisa xxmaj krueger takes a shot at it , using a light approach to</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos i do not think i am alone when i say that 2005 has not been particularly kind to the horror genre . xxmaj while \" cursed \" , \" hide and xxmaj seek \" , \" the xxmaj ring xxmaj two \" , and \" the xxmaj amityville xxmaj horror \" all showed glimpses of interest and potential , there have been more misses than hits . xxmaj for proof , see : \" white xxmaj noise \" , \" boogeyman \" , \" the xxmaj jacket \" , \" mindhunters \" , and \" alone in the xxmaj dark \" . xxmaj imagine my surprise when \" house of xxmaj wax \" , tightly written by siblings xxmaj chad and xxmaj carey xxmaj hayes , turned out to be … well , a surprise . \\n\\n xxmaj carly xxmaj jones ( elisha xxmaj cuthbert ) is a young</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj it is impossible to avoid comparing xxmaj zhang xxmaj yimou 's ` hero ' to xxmaj ang xxmaj lee 's ` crouching xxmaj tiger , xxmaj hidden xxmaj dragon . ' xxmaj they were both big - budget xxmaj chinese kung - fu films with breathtaking cinematography of xxmaj chinese landscape and a cast of super - stars . xxmaj but aside from the obvious , there is in fact nothing else to compare . ` hero ' fails to deliver on almost every level that really matters , proving that big - name stars , beautiful scenery , and action effects are no replacement for a director 's artistry and vision . \\n\\n xxmaj all the marketing hype preceding the premier of ` hero ' has done nothing more than make its failure a spectacular one . xxmaj much anticipated , ` hero ' drew movie -</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos i was xxunk most of the xxunk not born in the 80 's . i was born on may 14th 1994 . xxmaj despite this , my life was very much in the style of the 80 's . xxmaj when other kids had xxunk , i was playing xxmaj zelda on my xxup nes etc . xxmaj now , this movie holds a special place in my heart already despite me being only 15 years old at the time of writing this review . xxup i , because of my 80 's style early xxmaj childhood , watched many xxup tv shows and saw xxmaj many movies that other kids did n't see , and this movie was one of those , and one of the greatest too . \\n\\n xxmaj it starts off in the xxmaj los xxmaj angeles home of xxmaj alvin xxmaj seville , xxmaj</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj i 'm an atheist . xxmaj to me history and truth mean a lot . \\n\\n xxmaj this film is made after a novel published in 1921 , which is still being updated up to this day as if it was a history book . xxmaj well it 's not . xxmaj the movie is about the novels 1950s version . xxmaj some actors were xxup great but that does n't cover the plot . \\n\\n xxmaj in short man invents a super - bomb so xxmaj god and his friends hold a tribunal to see if they must intervene . xxmaj the devil analogy xxunk man , and for defense we have the spirit of man . xxmaj what is the spirit of man anyway ? xxmaj and why was the first defendant xxmaj adam ? xxmaj eventually you just get xxup us xxmaj christian propaganda in</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#显示分类结果\n",
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qteqUoLDJgg"
   },
   "source": [
    "Now, you have a text classification model that can accuractely predict if a movie review has positive or negative sentiment based on the raw text content of the review alone. With an understanding of the `fastai` APIs, you should now be able to implement your own text classifier on a dataset of your choice.\n",
    "\n",
    "While IMDB itself was fairly simple, many NLP problems today can be formulated a text classification problems. Some of the things you can do with text classifcation include:\n",
    "\n",
    "- Predicting the programming language of some source code\n",
    "- Building a simple email spam classifier\n",
    "- Improving the functionality of an automated content moderation bot for online chats or forums\n",
    "- Categorize documents based on their language footnote:[To do this well, you need a powerful tokenizer that can recognize text encoding in many languages]\n",
    "\n",
    "One of the best parts about text classification is that there's a single, simple, interpretable metric to optimize - accuracy. So not only can you solve these tasks, but you can also know how well you're doing using statistics that many people are familiar with.\n",
    "\n",
    "While the IMDB model we built just now does a wonderful job, it's perhaps not super impressive. We've had spam classifiers that do pretty good since the dawn of the Dinosaurs, so binary predictions on text is not something you might associate with the glorious future we sold you on in [[ch01]]. But it turns out that this idea of a language model is so powerful that it has become the poster child for NLP today.\n",
    "\n",
    "To illustrate this, let's give a language model on it's own, with no additional training or fine-tuning, a change to flex it's muscles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63VYYEhJDJgg"
   },
   "source": [
    "## Inference with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L75D816zDJgg"
   },
   "source": [
    "Now that we know how to train language models, we could conceptually train a very, very large one on a lot of data, and get it to produce very accurate sounding text. Here, we'll use the huggingface library to get prediction samples from a language model that was trained using a procedure similar to the one we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUTw1HYvDJgh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode a text inputs\n",
    "text = \"With great power comes great \"\n",
    "indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "# Convert indexed tokens in a PyTorch tensor\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avcMB9zJDJgh"
   },
   "source": [
    "The code snippet above initializes a tokenizer, which is function thats strings as input and returns arrays of numbers that are easier for the model to interpret. We'll be covering tokenizers in much more detail in the next chapter, but for now, if you want a quick look into what our model sees, try printing `tokens_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W5VTeLODJgh"
   },
   "outputs": [],
   "source": [
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxqgZqepDJgh"
   },
   "source": [
    "Now, let's do the actual inference, which is, again, just a few lines of code thanks to the amazing huggingface transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMMvOUrZDJgi"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# get the predicted next sub-word\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-zZiJdODJgi"
   },
   "source": [
    "Nice! It looks like whatever we just ran, in just a few lines of code, was able to recreate the wisdom of uncle Ben!footnote[A character from the Spider-Man comics book series, who once said \"with great power comes great responsibility,\" just like our language model did!]\n",
    "\n",
    "And to be clear, this wasn't just some simple lookup, database search, or something like that. This was an actual state-of-the-art neural network that after reading large amounts of text on the internet, is able to complete senences based in the \"knowledge\" it gained. Pretty cool, huh?\n",
    "\n",
    "But without context, this is all just a black box that you throw sentences into. So now, let's break down each line code in the block we just ran to really get a good idea of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3_zNPElDJgi"
   },
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PUl3gq4DJgj"
   },
   "source": [
    "First, we load a pretrained model. This is the single most important step for transfer learning. It downloads the model that we're going to use to make predictions from somewhere on the internet and loads it in the right format into  an object in our code. All of that functionality is thankfully packed into this one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE9Kkx8ODJgj"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PspZZkEbDJgj"
   },
   "source": [
    "Most deep learning libraries package this model loading functionality neatly into a simple function. It's the last thing you'll have to worry about.\n",
    "\n",
    "The specific model we're loading here is unimportant at this stage, but just so you know, it's called \"GPT2\", which was really revolutionary when it came out and basically broke the internet. You can read more about it from [an article](https://blog.floydhub.com/gpt2/) that Ajay wrote in 2019, but we'll talk about in this book as well, in [[ch09]]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD410L-QDJgj"
   },
   "source": [
    ".Loading Models\n",
    "> **Note:** Loading models to a variable named +model+, regardless of the task or domain is something that's extremely common in deep learning, so keep that in mind when you're browsing notebooks or code samples online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCe09qePDJgj"
   },
   "source": [
    "Next we run this little line of code, which tells our model that we're not training now and are instead going to make predictions (i.e., perform inference). There are a few things that change internally in the `model` object when we call this linefootmote:[Primarily, we disable the DropOut and BatchNorm layers, which are only useful during training], allowing us to generate predictions from the `model`. Again, this is not the most important line for what we're doing now, but make sure that you call this function whenever you would like to generate predictions. Running this line in a notebook will also print out all the layers of the model in the standard PyTorch format, so maybe scroll through that if you're feeling curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB1czK-nDJgk"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FVpdbk-DJgk"
   },
   "source": [
    "With the weights downloaded, modedl loaded into memory, and the `model` object set into evaluation mode, It's time to crank out some output from our lean footnote:[Ok, maybe this phrase is not applicable to GPT-2 specifically, but when we all have computers that are 300 times faster than what we have today, this adjective will be accurate.], mean, text generating machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKWmV7IkDJgk"
   },
   "source": [
    "### Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5-kA8lZDJgk"
   },
   "source": [
    "We're going to group the next three lines together, since they work as a block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msMKtu1KDJgl"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p0gdkJhDJgl"
   },
   "source": [
    "The first line, `with torch.no_grad():` tells PyTorch to run the the lines in that indent block in the `torch.no_grad()` context, which means PyTorch won't calculate the gradients, or backward pass, for the model. If you're not familiar with backpropogation, or not entirely clear why gradients are calculated in the first place, refer to the resources we have in the introduction. Strictly speaking, we don't _need_ to turn off gradient computation, but this saves time, memory, and compute, and makes the inference run faster.\n",
    "\n",
    "In the `torch.no_grad()` context, we then run a forward pass. As always, PyTorch makes this extremely simple. Just call `model` as a function, with the `tokens_tensor` we prepared above as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_KEmtGyDJgl"
   },
   "source": [
    "But wait, wasn't +model+ an object with the pretrained weights that we loaded above? How is it also a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSSkykUwDJgm"
   },
   "source": [
    ".Python Dunder Methods\n",
    "****\n",
    "\n",
    "In Python, you can actually do this! You have to define `__call__` method in your class, which is a special function called a dunder method. Python has a lot of these cool dunder functions, some of which you've likely encounter before, like `__init__`, which let's you set up a constructor for your class, and `__len__`, which let's you define a \"length\" property for your objects which you can access via the `len()` function. Python dunder methods allow you to define a lot of cool functionality for your custom classes, such as addition, equality, and more.\n",
    "\n",
    "If you define a function called `__call__()` in your Python class, you can then treat instances of your class as functions, and the `__call__()` function will be invoked everytime you do so. We'll soon talk about PyTorch `nn.Module` objects, which are the building blocks for neural nets. The `nn.Module` class implements the `__call__` function by default. Therefore, every PyTorch model (and submodule) can also be called as a function, which can make your code very neat and tidy. This is why we can both define the +model+ variable and call it as we would for a function at the same time.\n",
    "\n",
    "If you're interested in learning more about python dunder methods, check out this [tutorial](https://rszalski.github.io/magicmethods/) or read more online (there are plenty of great resources one search away).\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFyvesSdDJgm"
   },
   "source": [
    "Calling `model(input_tensor)`, in general, will return a `torch.tensor` object with the predictions. But in this case, the huggingface library actually gives us a lot of other items as well. In this case, `model(tokens_tensor)` will return a tuple, where the first element is the predictions tensor. Let's quickly confirm alll of this by checking a few lengths and shapes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "OxsDCw6aDJgm"
   },
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hiXqXGkDJgm"
   },
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKfwVPwaDJgn"
   },
   "source": [
    "This checks out, because according to the huggingface transformers documentation, the predictions tensor is supposed to have shape `(batch_size, sequence_length, config.vocab_size)`. Here, the batch size is 1, since we're only passing in one sentence. The sequence length should be 5, which makes sense if you take a look at the line where we define the input sentence, which had five words (space delimitted substrings) in the string:\n",
    "\n",
    "```\n",
    "text = \"With great power comes great \"\n",
    "```\n",
    "\n",
    "The value of 50257 for the vocabulary size seems accurate, but this is something we could always double-check by going through the documentation for this model.\n",
    "\n",
    "\n",
    "> **Tip:** We can't emphasize enough how much this technique of checking the size, shape, and dimensions of `torch.tensors` is. It's one of the most effective ways of debugging your model. Hopefully, as you start training more complex models and building your own architectures from scratch, this will come naturally. But until then, always remember to try check the size with `.size` and reason through what's going on in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NJ8iZLwDJgn"
   },
   "source": [
    "Since it seems like `outputs[0]` is what we want, we'll assign to the variable `predictions`. Putting these together and wrapping them in the `torch.no_grad()` context gives us that mini-block of code that we had above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_2ELQxBDJgn"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOR5HQKjDJgn"
   },
   "source": [
    "`predictions` is a `torch.tensor` that has values that describe the probability of each word. Remember, one of the dimensions of this `torch.tensor` is the size of the vocabulary (i.e. the number of possible words that the model could predict). What we want now is the word that is mostly likey to come next in out sentence, We grab this by using the `argmax` function, which gets the index of the largest value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LONJGzyADJgo"
   },
   "outputs": [],
   "source": [
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3orgZICDJgo"
   },
   "source": [
    "To ensure that we're absolutely clear on the what exactly we're doing, let's also quickly break down the way we index `predictions`. It's a three dimensional tensor, so we specify 3 indices. The first, along the batch dimension, is `0`. Since we're not running batch predictions, there's only one element in this axis, so it's what we pick. Along the sequence length dimension, we pick the last element. This is because we want to predict the last word in the sentence we passed in. The last index is `:`, which means we want to grab everything. We need all the elements along the vocabulary dimension to calculate which one is most likely.\n",
    "\n",
    "Finally, we decode the index we got into a word using the +tokenizer.decode()+ funtion. This is just a simple lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_nWgztDDJgo"
   },
   "outputs": [],
   "source": [
    "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctvrf8JLDJgp"
   },
   "source": [
    "And there we have it! Recreating wisdom in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8dBBiiJDJgp"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRL7aoh7DJgp"
   },
   "source": [
    "In [[ch08]] we'll try putting these ideas together to develop a technique that utilizes both transformers and transfer learning together to create an incredibly powerful set of models that can solve these tasks we just demonstrated as well as many more.\n",
    "    \n",
    "There's a lot in this chapter that we haven't explained yet. We've intentionally left out a lot of details such as what exactly a model is/does, how the tokenizer is implemented in code, and perhaps most importantly, how to use the pretrained model for transfer learning.\n",
    "\n",
    "Don't worry though: we'll eventually get to all that. The goal of this chapter was to help you understand some of the important components of an NLP pipeline by running code and seeing results in real time. To test your understanding of the material so far, try to use a different language model, swap out the prompt, and see if you can get the model to predict a popular quote, phrase, or idiom. Note that to do this, you might need to swap out the tokenizer as well.\n",
    "    \n",
    "Once you're able to perform these tasks, you should be ready to move on to the next chapter, in which we formally introduce some of the most popular NLP applications today and build a few together."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
